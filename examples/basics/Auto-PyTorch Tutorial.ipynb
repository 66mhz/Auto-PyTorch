{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial introduces the basic Auto-PyTorch API together with the classes for featurized and image data.\n",
    "So far, Auto-PyTorch covers classification and regression on featurized data as well as classification on image data.\n",
    "For installing Auto-PyTorch, please refer to the github page.\n",
    "\n",
    "**Disclaimer**: In this notebook, data will be downloaded from the openml project for featurized tasks and CIFAR10 will be downloaded for image classification. Hence, an internet connection is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "There are classes for featurized tasks (classification, multi-label classification, regression) and image tasks (classification). You can import them via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoPyTorch import (AutoNetClassification,\n",
    "                         AutoNetMultilabel,\n",
    "                         AutoNetRegression,\n",
    "                         AutoNetImageClassification,\n",
    "                         AutoNetImageClassificationMultipleDatasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports for later usage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "import openml\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initialization of a class, you can specify its configuration. Later, you can override its configuration in each fit call. The *config_preset* allows to constrain the search space to one of *tiny_cs, medium_cs* or *full_cs*. These presets can be seen in *core/presets/*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "autonet = AutoNetClassification(config_preset=\"tiny_cs\", result_logger_dir=\"logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful methods provided by the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the current configuration as dict\n",
    "current_configuration = autonet.get_current_autonet_config()\n",
    "\n",
    "# Get the ConfigSpace object with all hyperparameters, conditions, default values and default ranges\n",
    "hyperparameter_search_space = autonet.get_hyperparameter_search_space()\n",
    "\n",
    "# Print all possible configuration options \n",
    "#autonet.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The most important methods for using Auto-PyTorch are ***fit***, ***refit***, ***score*** and ***predict***.\n",
    "\n",
    "First, we get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data from the openml task \"Supervised Classification on credit-g (https://www.openml.org/t/31)\"\n",
    "task = openml.tasks.get_task(task_id=31)\n",
    "X, y = task.get_X_and_y()\n",
    "ind_train, ind_test = task.get_train_test_split_indices()\n",
    "X_train, Y_train = X[ind_train], y[ind_train]\n",
    "X_test, Y_test = X[ind_test], y[ind_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***fit*** is used to search for a good configuration by fitting configurations chosen by the algorithm (by default BOHB). The incumbent configuration is then returned and stored in the class.\n",
    "\n",
    "We recommend to have a look at the possible configuration options first. Some of the most important options allow you to set the budget type (epochs or time), run id and task id for cluster usage, tensorboard logging, seed and more.\n",
    "\n",
    "Here we search for a configuration for 300 seconds with 60-100 s time for fitting each individual configuration.\n",
    "Use the *validation_split* parameter to specify a split size. You can also pass your own validation set\n",
    "via *X_val* and *Y_val*. Use *log_level=\"info\"* or *log_level=\"debug\"* for more detailed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:484: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:484: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:484: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:484: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "autonet = AutoNetClassification(config_preset=\"tiny_cs\", result_logger_dir=\"logs/\")\n",
    "# Fit (note that the settings are for demonstration, you might need larger budgets)\n",
    "results_fit = autonet.fit(X_train=X_train,\n",
    "                          Y_train=Y_train,\n",
    "                          validation_split=0.3,\n",
    "                          max_runtime=300,\n",
    "                          min_budget=60,\n",
    "                          max_budget=100,\n",
    "                          refit=True)\n",
    "\n",
    "# Save fit results as json\n",
    "with open(\"logs/results_fit.json\", \"w\") as file:\n",
    "    json.dump(results_fit, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***refit*** allows you to fit a configuration of your choice for a defined time. By default, the incumbent configuration is refitted during a *fit* call using the *max_budget*. However, *refit* might be useful if you want to fit on the full dataset or even another dataset or if you just want to fit a model without searching.\n",
    "\n",
    "You can specify a hyperparameter configuration to fit (if you do not specify a configuration the incumbent configuration from the last fit call will be used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:50:24 Start autonet with config:\n",
      "{'embeddings': ['none'], 'lr_scheduler': ['cosine_annealing', 'plateau'], 'networks': ['shapedresnet'], 'over_sampling_methods': ['smote'], 'preprocessors': ['none', 'truncated_svd', 'power_transformer'], 'target_size_strategies': ['none', 'upsample', 'median'], 'result_logger_dir': 'logs/', 'budget_type': 'epochs', 'log_level': 'info', 'use_tensorboard_logger': True, 'validation_split': 0.0, 'hyperparameter_search_space_updates': None, 'categorical_features': None, 'dataset_name': None, 'run_id': '0', 'task_id': -1, 'algorithm': 'bohb', 'portfolio_type': 'greedy', 'eta': 3, 'min_workers': 1, 'working_dir': '.', 'network_interface_name': 'eth0', 'memory_limit_mb': 1000000, 'run_worker_on_master_node': True, 'use_pynisher': True, 'refit_validation_split': 0.0, 'cross_validator': 'none', 'cross_validator_args': {}, 'min_budget_for_cv': 0, 'shuffle': True, 'imputation_strategies': ['mean', 'median', 'most_frequent'], 'normalization_strategies': ['none', 'minmax', 'standardize', 'maxabs'], 'under_sampling_methods': ['none', 'random'], 'final_activation': 'softmax', 'initialization_methods': ['default', 'sparse'], 'initializer': 'simple_initializer', 'optimizer': ['adam', 'adamw', 'sgd', 'rmsprop'], 'additional_logs': [], 'optimize_metric': 'accuracy', 'additional_metrics': [], 'loss_modules': ['cross_entropy', 'cross_entropy_weighted'], 'batch_loss_computation_techniques': ['standard', 'mixup'], 'cuda': True, 'torch_num_threads': 1, 'full_eval_each_epoch': False, 'best_over_epochs': False, 'save_models': False, 'predict_model': None, 'early_stopping_patience': inf, 'early_stopping_reset_parameters': False, 'random_seed': 2458212657, 'min_budget': 5, 'max_budget': 150, 'max_runtime': inf, 'num_iterations': 4, 'cv_splits': 1, 'increase_number_of_trained_datasets': False}\n",
      "15:50:24 Start Refitting\n",
      "15:50:24 [AutoNet] No validation set given and either no cross validator given or budget too low for CV. Continue by splitting 0 of training data.\n",
      "15:50:24 [AutoNet] CV split 0 of 1\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/numpy/core/_methods.py:195: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "15:50:44 Finished train with budget 50.0: Preprocessing took 0s, Training took 20s, Wrap up took 0s. Total time consumption in s: 20\n",
      "15:50:45 [AutoNet] Done with current split!\n",
      "15:50:45 Aggregate the results across the splits\n",
      "15:50:45 Process 1 additional result(s)\n",
      "15:50:45 Done Refitting\n"
     ]
    }
   ],
   "source": [
    "# Create an autonet\n",
    "autonet_config = {\n",
    "    \"result_logger_dir\" : \"logs/\",\n",
    "    \"budget_type\" : \"epochs\",\n",
    "    \"log_level\" : \"info\", \n",
    "    \"use_tensorboard_logger\" : True,\n",
    "    \"validation_split\" : 0.0\n",
    "    }\n",
    "autonet = AutoNetClassification(**autonet_config)\n",
    "\n",
    "# Sample a random hyperparameter configuration as an example\n",
    "hyperparameter_config = autonet.get_hyperparameter_search_space().sample_configuration().get_dictionary()\n",
    "\n",
    "# Refit with sampled hyperparameter config for 120 s. This time on the full dataset.\n",
    "results_refit = autonet.refit(X_train=X_train,\n",
    "                              Y_train=Y_train,\n",
    "                              X_valid=None,\n",
    "                              Y_valid=None,\n",
    "                              hyperparameter_config=hyperparameter_config,\n",
    "                              autonet_config=autonet.get_current_autonet_config(),\n",
    "                              budget=50)\n",
    "\n",
    "# Save json\n",
    "with open(\"logs/results_refit.json\", \"w\") as file:\n",
    "    json.dump(results_refit, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***pred*** returns the predictions of the incumbent model. ***score*** can be used to evaluate the model on a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Accuracy score 75.0\n"
     ]
    }
   ],
   "source": [
    "# See how the random configuration performs (often it just predicts 0)\n",
    "score = autonet.score(X_test=X_test, Y_test=Y_test)\n",
    "pred = autonet.predict(X=X_test)\n",
    "\n",
    "print(\"Model prediction:\", pred[0:10])\n",
    "print(\"Accuracy score\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also get the incumbent model as PyTorch Sequential model via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=36, bias=True)\n",
      "  (1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=36, out_features=52, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=36, out_features=52, bias=True)\n",
      "        (1): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=52, out_features=52, bias=True)\n",
      "      )\n",
      "      (shake_shake_layers): Sequential(\n",
      "        (0): Linear(in_features=36, out_features=52, bias=True)\n",
      "        (1): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=52, out_features=52, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=52, out_features=69, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=52, out_features=69, bias=True)\n",
      "        (1): BatchNorm1d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=69, out_features=69, bias=True)\n",
      "      )\n",
      "      (shake_shake_layers): Sequential(\n",
      "        (0): Linear(in_features=52, out_features=69, bias=True)\n",
      "        (1): BatchNorm1d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=69, out_features=69, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=69, out_features=53, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=69, out_features=53, bias=True)\n",
      "        (1): BatchNorm1d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=53, out_features=53, bias=True)\n",
      "      )\n",
      "      (shake_shake_layers): Sequential(\n",
      "        (0): Linear(in_features=69, out_features=53, bias=True)\n",
      "        (1): BatchNorm1d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=53, out_features=53, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=53, out_features=37, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=53, out_features=37, bias=True)\n",
      "        (1): BatchNorm1d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=37, out_features=37, bias=True)\n",
      "      )\n",
      "      (shake_shake_layers): Sequential(\n",
      "        (0): Linear(in_features=53, out_features=37, bias=True)\n",
      "        (1): BatchNorm1d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=37, out_features=37, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (shortcut): Linear(in_features=37, out_features=21, bias=True)\n",
      "      (start_norm): Sequential(\n",
      "        (0): BatchNorm1d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=37, out_features=21, bias=True)\n",
      "        (1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=21, out_features=21, bias=True)\n",
      "      )\n",
      "      (shake_shake_layers): Sequential(\n",
      "        (0): Linear(in_features=37, out_features=21, bias=True)\n",
      "        (1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=21, out_features=21, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=21, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = autonet.get_pytorch_model()\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurized Data\n",
    "\n",
    "All classes for featurized data (*AutoNetClassification*, *AutoNetMultilabel*, *AutoNetRegression*) can be used as in the example above. The only difference is the type of labels they accept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data\n",
    "\n",
    "Auto-PyTorch provides two classes for image data. *autonet_image_classification* can be used for classification for images. The *autonet_multi_image_classification* class allows to search for configurations for image classification across multiple datasets. This means Auto-PyTorch will try to choose a configuration that works well on all given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classes\n",
    "autonet_image_classification = AutoNetImageClassification(config_preset=\"full_cs\", result_logger_dir=\"logs/\")\n",
    "autonet_multi_image_classification = AutoNetImageClassificationMultipleDatasets(config_preset=\"tiny_cs\", result_logger_dir=\"logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For passing your image data, you have two options (note that arrays are expected):\n",
    "\n",
    "I) Via a path to a comma-separated value file, which in turn contains the paths to the images and the image labels (note header is assumed to be None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.abspath(\"../../datasets/example.csv\")\n",
    "\n",
    "X_train = np.array([csv_dir])\n",
    "Y_train = np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) directly passing the paths to the images and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_dir, header=None)\n",
    "X_train = df.values[:,0]\n",
    "Y_train = df.values[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Make sure you specify *image_root_folders* if the paths to the images are not specified from your current working directory. You can also specify *images_shape* to up- or downscale images.\n",
    "\n",
    "Using the flag *save_checkpoints=True* will save checkpoints to the result directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process pynisher function call:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/pynisher/limit_function_call.py\", line 93, in subprocess_func\n",
      "    return_value = ((func(*args, **kwargs), 0))\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/core/worker_no_timelimit.py\", line 122, in optimize_pipeline\n",
      "    raise e\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/core/worker_no_timelimit.py\", line 116, in optimize_pipeline\n",
      "    config_id=config_id, working_directory=self.working_directory), random.getstate()\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/base/pipeline.py\", line 60, in fit_pipeline\n",
      "    return self.root.fit_traverse(**kwargs)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/base/node.py\", line 115, in fit_traverse\n",
      "    node.fit_output = node.fit(**required_kwargs)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/nodes/image/single_dataset.py\", line 21, in fit\n",
      "    budget=budget, budget_type=budget_type, config_id=config_id, working_directory=working_directory)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/base/pipeline.py\", line 60, in fit_pipeline\n",
      "    return self.root.fit_traverse(**kwargs)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/base/node.py\", line 115, in fit_traverse\n",
      "    node.fit_output = node.fit(**required_kwargs)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/nodes/image/cross_validation_indices.py\", line 144, in fit\n",
      "    working_directory=working_directory)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/base/pipeline.py\", line 60, in fit_pipeline\n",
      "    return self.root.fit_traverse(**kwargs)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/base/node.py\", line 115, in fit_traverse\n",
      "    node.fit_output = node.fit(**required_kwargs)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/pipeline/nodes/image/simple_train_node.py\", line 135, in fit\n",
      "    optimize_metric_results, train_loss, stop_training = trainer.train(epoch + 1, train_loader, optimize_metrics)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/components/training/image/trainer.py\", line 90, in train\n",
      "    loss.backward()\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n",
      "15:50:49 job (0, 0, 0) failed with exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/hpbandster/core/worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/autoPyTorch-0.0.2-py3.6.egg/autoPyTorch/core/worker_no_timelimit.py\", line 60, in compute\n",
      "    result, randomstate = limit_train(config, budget, config_id, random.getstate())\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zimmerl/Auto-PyTorch_releases/Auto-PyTorch/env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "autonet_image_classification.fit(X_train=X_train,\n",
    "                                 Y_train=Y_train,\n",
    "                                 images_shape=[3,32,32],\n",
    "                                 min_budget=200,\n",
    "                                 max_budget=400,\n",
    "                                 max_runtime=600,\n",
    "                                 save_checkpoints=True,\n",
    "                                 images_root_folders=[os.path.abspath(\"../../datasets/example_images\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-PyTorch also supports some common datasets. By passing a comma-separated value file with just one line, e.g. \"CIFAR10, 0\" and specifying *default_dataset_download_dir* it will automatically download the data and use it for searching. Supported datasets are CIFAR10, CIFAR100, SVHN and MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_to_cifar_csv = os.path.abspath(\"../../datasets/CIFAR10.csv\")\n",
    "\n",
    "autonet_image_classification.fit(X_train=np.array([path_to_cifar_csv]),\n",
    "                                 Y_train=np.array([0]),\n",
    "                                 min_budget=600,\n",
    "                                 max_budget=900,\n",
    "                                 max_runtime=1800,\n",
    "                                 default_dataset_download_dir=\"./datasets\",\n",
    "                                 images_root_folders=[\"./datasets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For searching across multiple datasets, pass multiple csv files to the corresponding Auto-PyTorch class. Make sure your specify *images_root_folders* for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonet_multi_image_classification.fit(X_train=np.array([path_to_cifar_csv, csv_dir]),\n",
    "                                       Y_train=np.array([0]),\n",
    "                                       min_budget=1500,\n",
    "                                       max_budget=2000,\n",
    "                                       max_runtime=4000,\n",
    "                                       default_dataset_download_dir=\"./datasets\",\n",
    "                                       images_root_folders=[\"./datasets\", \"./datasets/example_images\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
